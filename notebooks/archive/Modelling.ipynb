{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'external', 'interim', 'processed', 'raw']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#set working directory to this from the workspace\n",
    "os.chdir('/Users/janjarco/Programming/PrivateRepository/FlightDataThesisProject')\n",
    "\n",
    "#list all the files in current working directory\n",
    "print(os.listdir(\"data\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the merged data frame of orders and clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/cz1c1p2d6ln8dw3vyy038cwm0000gn/T/ipykernel_89154/192662082.py:3: DtypeWarning: Columns (83,116,123,124,126,129,130,131,132,133,156,157) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/processed/clicks_orders_merge_filtered_currency.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed (in seconds): 2.453141927719116\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "df = pd.read_csv(\"data/processed/clicks_orders_merge_filtered_currency.csv\")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Time elapsed (in seconds):', elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df.columns.values  by regex\n",
    "def filter_columns_by_regex(df, regex):\n",
    "    import re\n",
    "    return [col for col in df.columns.values if bool(re.search(pattern=str(regex), string=str(col)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orders_search_data.experiments.202206-direct-to-step2',\n",
       " 'orders_search_data.experiments.202212-new-sorting',\n",
       " 'orders_search_data.experiments.202301-conditions-text',\n",
       " 'orders_search_data.experiments.202204-danish-explanations',\n",
       " 'orders_search_data.experiments.202206-sabre-gateway-migration',\n",
       " 'orders_search_data.experiments.202202-test-experiment',\n",
       " 'orders_search_data.experiments.202202-handbag-ordering',\n",
       " 'orders_search_data.experiments.202203-german-background-image',\n",
       " 'orders_search_data.experiments.202203-mix-classic',\n",
       " 'orders_search_data.experiments.202205-meta-per-pax']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_columns_by_regex(df, 'experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_created_at_date\n",
       "2022-01-01     96\n",
       "2022-01-02    154\n",
       "2022-01-03    149\n",
       "2022-01-04    149\n",
       "2022-01-05    145\n",
       "             ... \n",
       "2023-03-16    352\n",
       "2023-03-17    329\n",
       "2023-03-18    326\n",
       "2023-03-19    482\n",
       "2023-03-20    223\n",
       "Name: clicks_index, Length: 444, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize df by orders_created_at_date and caluculate counts \n",
    "df.groupby([\"orders_created_at_date\"])['clicks_index'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total price in SEK\n",
    "df['orders_total_price_SEK'] = df['clicks_itinerary_sales_price'] * df['exchange_rate']\n",
    "# df['orders_total_price_SEK'].hist(bins=100)\n",
    "\n",
    "# histogram of clicks_itinerary_travel_time in hours\n",
    "df['clicks_itinerary_travel_timehours'] = round(df['clicks_itinerary_travel_time'] / 60,2)\n",
    "# df['clicks_itinerary_travel_timehours'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orders_search_data.experiments.202206-direct-to-step2',\n",
       " 'orders_search_data.experiments.202212-new-sorting',\n",
       " 'orders_search_data.experiments.202301-conditions-text',\n",
       " 'orders_search_data.experiments.202204-danish-explanations',\n",
       " 'orders_search_data.experiments.202206-sabre-gateway-migration',\n",
       " 'orders_search_data.experiments.202202-test-experiment',\n",
       " 'orders_search_data.experiments.202202-handbag-ordering',\n",
       " 'orders_search_data.experiments.202203-german-background-image',\n",
       " 'orders_search_data.experiments.202203-mix-classic',\n",
       " 'orders_search_data.experiments.202205-meta-per-pax']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_vars = filter_columns_by_regex(df, '.*experiments')\n",
    "experiment_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as stats\n",
    "\n",
    "def perform_t_test(dataframe, group_var, var_test, group1, group2):\n",
    "    print(\"test performed for \"+group_var)\n",
    "    # Define the two groups to compare\n",
    "    group1_data = dataframe.loc[dataframe[group_var] == group1, var_test]\n",
    "    group2_data = dataframe.loc[dataframe[group_var] == group2, var_test]\n",
    "\n",
    "    # Perform a two-sample t-test assuming unequal variances\n",
    "    t_stat, p_value = stats.ttest_ind(group1_data, group2_data, equal_var=False)\n",
    "\n",
    "    # Calculate the mean and count for each group\n",
    "    group1_mean = group1_data.mean()\n",
    "    group1_count = group1_data.count()\n",
    "    group2_mean = group2_data.mean()\n",
    "    group2_count = group2_data.count()\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Group 1 mean:\", group1_mean, \"count:\", group1_count)\n",
    "    print(\"Group 2 mean:\", group2_mean, \"count:\", group2_count)\n",
    "    \n",
    "    # print(\"t-statistic:\", t_stat)\n",
    "    print(\"p-value:\", p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performed for orders_search_data.experiments.202206-direct-to-step2\n",
      "Group 1 mean: 5264.466991171823 count: 100854\n",
      "Group 2 mean: nan count: 0\n",
      "p-value: nan\n",
      "test performed for orders_search_data.experiments.202212-new-sorting\n",
      "Group 1 mean: 4894.973321161475 count: 1926\n",
      "Group 2 mean: 5120.744760110653 count: 34296\n",
      "p-value: 0.19021862110009982\n",
      "test performed for orders_search_data.experiments.202301-conditions-text\n",
      "Group 1 mean: 6469.570812338355 count: 4448\n",
      "Group 2 mean: 6439.887532140018 count: 4428\n",
      "p-value: 0.888474767522465\n",
      "test performed for orders_search_data.experiments.202204-danish-explanations\n",
      "Group 1 mean: 5234.50480009638 count: 6298\n",
      "Group 2 mean: 5334.827107774125 count: 6083\n",
      "p-value: 0.3517088608483724\n",
      "test performed for orders_search_data.experiments.202206-sabre-gateway-migration\n",
      "Group 1 mean: 6339.656490773792 count: 1450\n",
      "Group 2 mean: 6257.456313094394 count: 1409\n",
      "p-value: 0.8116251412140219\n",
      "test performed for orders_search_data.experiments.202202-test-experiment\n",
      "Group 1 mean: 5511.191509511906 count: 8105\n",
      "Group 2 mean: 5000.291003151884 count: 4431\n",
      "p-value: 3.995848562116653e-05\n",
      "test performed for orders_search_data.experiments.202202-handbag-ordering\n",
      "Group 1 mean: 5058.446855307271 count: 2008\n",
      "Group 2 mean: 4943.178244180623 count: 2054\n",
      "p-value: 0.578190597868627\n",
      "test performed for orders_search_data.experiments.202203-german-background-image\n",
      "Group 1 mean: 5975.236763986333 count: 3000\n",
      "Group 2 mean: 5740.044085903606 count: 2801\n",
      "p-value: 0.2143857360618735\n",
      "test performed for orders_search_data.experiments.202203-mix-classic\n",
      "Group 1 mean: 7208.231630154547 count: 110\n",
      "Group 2 mean: 5897.991622551695 count: 3627\n",
      "p-value: 0.10952386463197245\n",
      "test performed for orders_search_data.experiments.202205-meta-per-pax\n",
      "Group 1 mean: 5182.928321238557 count: 721\n",
      "Group 2 mean: 4726.196599993207 count: 736\n",
      "p-value: 0.2159843583793229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for var in experiment_vars:\n",
    "    perform_t_test(\n",
    "    dataframe=df, \n",
    "    group_var=var, \n",
    "    var_test = 'orders_total_price_SEK', \n",
    "    group1='control', \n",
    "    group2 = 'hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "addon_vars = filter_columns_by_regex(df, 'orders_addon')\n",
    "# from a list of variables addon_vars create a sum of all the variables\n",
    "df['orders_addon_totalsum'] = df[addon_vars].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test performed for orders_search_data.experiments.202206-direct-to-step2\n",
      "Group 1 mean: 0.4750233010093799 count: 100854\n",
      "Group 2 mean: nan count: 0\n",
      "p-value: nan\n",
      "test performed for orders_search_data.experiments.202212-new-sorting\n",
      "Group 1 mean: 0.48286604361370716 count: 1926\n",
      "Group 2 mean: 0.46745976207137857 count: 34296\n",
      "p-value: 0.3656469765328353\n",
      "test performed for orders_search_data.experiments.202301-conditions-text\n",
      "Group 1 mean: 0.4660521582733813 count: 4448\n",
      "Group 2 mean: 0.480126467931346 count: 4428\n",
      "p-value: 0.3623097299108611\n",
      "test performed for orders_search_data.experiments.202204-danish-explanations\n",
      "Group 1 mean: 0.4785646236900603 count: 6298\n",
      "Group 2 mean: 0.4757520960052606 count: 6083\n",
      "p-value: 0.8347089132184946\n",
      "test performed for orders_search_data.experiments.202206-sabre-gateway-migration\n",
      "Group 1 mean: 0.44551724137931037 count: 1450\n",
      "Group 2 mean: 0.43789921930447123 count: 1409\n",
      "p-value: 0.7838634479036116\n",
      "test performed for orders_search_data.experiments.202202-test-experiment\n",
      "Group 1 mean: 0.47230104873534856 count: 8105\n",
      "Group 2 mean: 0.4558790340780862 count: 4431\n",
      "p-value: 0.22279536767009814\n",
      "test performed for orders_search_data.experiments.202202-handbag-ordering\n",
      "Group 1 mean: 0.4497011952191235 count: 2008\n",
      "Group 2 mean: 0.4294060370009737 count: 2054\n",
      "p-value: 0.3546953156832504\n",
      "test performed for orders_search_data.experiments.202203-german-background-image\n",
      "Group 1 mean: 0.49033333333333334 count: 3000\n",
      "Group 2 mean: 0.4691181720813995 count: 2801\n",
      "p-value: 0.28054874294796645\n",
      "test performed for orders_search_data.experiments.202203-mix-classic\n",
      "Group 1 mean: 0.6454545454545455 count: 110\n",
      "Group 2 mean: 0.4731182795698925 count: 3627\n",
      "p-value: 0.04299014488698466\n",
      "test performed for orders_search_data.experiments.202205-meta-per-pax\n",
      "Group 1 mean: 0.406380027739251 count: 721\n",
      "Group 2 mean: 0.41304347826086957 count: 736\n",
      "p-value: 0.8520268520898736\n"
     ]
    }
   ],
   "source": [
    "for var in experiment_vars:\n",
    "    perform_t_test(\n",
    "    dataframe=df, \n",
    "    group_var=var, \n",
    "    var_test = 'orders_addon_totalsum', \n",
    "    group1='control', \n",
    "    group2 = 'hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "search_engine_changes = {\n",
    "    'fix.reordering.after.maximum.results': datetime(2023, 1, 5, 11, 50),\n",
    "    'remove.itineraries.departure.too.close': datetime(2022, 9, 28, 11, 9),\n",
    "    'lhg.gateway.migration': datetime(2022, 7, 25, 11, 22),\n",
    "    'add.support.unacceptable.connections': datetime(2023, 2, 16, 12, 55),\n",
    "    'support.20ITINS': datetime(2022, 12, 21, 16, 23),\n",
    "    'avoid.cabin.downgrade': datetime(2022, 8, 19, 9, 14),\n",
    "    'bump.BFM.version': datetime(2022, 8, 12, 9, 29),\n",
    "    'gateway.migration': datetime(2022, 6, 13, 13, 44),\n",
    "    'hack.bagprice.override.Altea.FLX': datetime(2022, 9, 15, 7, 50),\n",
    "    'mixed.itineraries': datetime(2022, 8, 10, 11, 54),\n",
    "    'add.price.override': datetime(2022, 8, 10, 7, 23),\n",
    "    'mobile.pay.support.Denmark': datetime(2023, 2, 21, 11, 31),\n",
    "    'immediate.payment.if.less.36.hours.departure': datetime(2022, 9, 26, 13, 38)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# transform the 'clicks_created_at' column to datetime format\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mclicks_created_at\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mto_datetime(df[\u001b[39m'\u001b[39;49m\u001b[39mclicks_created_at\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      4\u001b[0m \u001b[39m# print the resulting dataframe\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mclicks_created_at\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:1068\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         result \u001b[39m=\u001b[39m arg\u001b[39m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1067\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1068\u001b[0m         values \u001b[39m=\u001b[39m convert_listlike(arg\u001b[39m.\u001b[39;49m_values, \u001b[39mformat\u001b[39;49m)\n\u001b[1;32m   1069\u001b[0m         result \u001b[39m=\u001b[39m arg\u001b[39m.\u001b[39m_constructor(values, index\u001b[39m=\u001b[39marg\u001b[39m.\u001b[39mindex, name\u001b[39m=\u001b[39marg\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1070\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[39m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:438\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m infer_datetime_format\n\u001b[1;32m    437\u001b[0m utc \u001b[39m=\u001b[39m tz \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutc\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 438\u001b[0m result, tz_parsed \u001b[39m=\u001b[39m objects_to_datetime64ns(\n\u001b[1;32m    439\u001b[0m     arg,\n\u001b[1;32m    440\u001b[0m     dayfirst\u001b[39m=\u001b[39;49mdayfirst,\n\u001b[1;32m    441\u001b[0m     yearfirst\u001b[39m=\u001b[39;49myearfirst,\n\u001b[1;32m    442\u001b[0m     utc\u001b[39m=\u001b[39;49mutc,\n\u001b[1;32m    443\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    444\u001b[0m     require_iso8601\u001b[39m=\u001b[39;49mrequire_iso8601,\n\u001b[1;32m    445\u001b[0m     allow_object\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    446\u001b[0m )\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m tz_parsed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    449\u001b[0m     \u001b[39m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[39m# is in UTC\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     dta \u001b[39m=\u001b[39m DatetimeArray(result, dtype\u001b[39m=\u001b[39mtz_to_dtype(tz_parsed))\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:2177\u001b[0m, in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[1;32m   2175\u001b[0m order: Literal[\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m flags\u001b[39m.\u001b[39mf_contiguous \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2177\u001b[0m     result, tz_parsed \u001b[39m=\u001b[39m tslib\u001b[39m.\u001b[39;49marray_to_datetime(\n\u001b[1;32m   2178\u001b[0m         data\u001b[39m.\u001b[39;49mravel(\u001b[39m\"\u001b[39;49m\u001b[39mK\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   2179\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   2180\u001b[0m         utc\u001b[39m=\u001b[39;49mutc,\n\u001b[1;32m   2181\u001b[0m         dayfirst\u001b[39m=\u001b[39;49mdayfirst,\n\u001b[1;32m   2182\u001b[0m         yearfirst\u001b[39m=\u001b[39;49myearfirst,\n\u001b[1;32m   2183\u001b[0m         require_iso8601\u001b[39m=\u001b[39;49mrequire_iso8601,\n\u001b[1;32m   2184\u001b[0m         allow_mixed\u001b[39m=\u001b[39;49mallow_mixed,\n\u001b[1;32m   2185\u001b[0m     )\n\u001b[1;32m   2186\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mreshape(data\u001b[39m.\u001b[39mshape, order\u001b[39m=\u001b[39morder)\n\u001b[1;32m   2187\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOverflowError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   2188\u001b[0m     \u001b[39m# Exception is raised when a part of date is greater than 32 bit signed int\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/_libs/tslib.pyx:427\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/_libs/tslib.pyx:524\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True"
     ]
    }
   ],
   "source": [
    "# transform the 'clicks_created_at' column to datetime format\n",
    "df['clicks_created_at'] = pd.to_datetime(df['clicks_created_at'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2022-01-01 02:36:08+01:00\n",
       "1         2022-01-01 06:46:22+01:00\n",
       "2         2022-01-01 07:35:47+01:00\n",
       "3         2022-01-01 07:44:40+01:00\n",
       "4         2022-01-01 08:24:28+01:00\n",
       "                    ...            \n",
       "135990    2023-03-20 16:47:05+01:00\n",
       "135991    2023-03-20 16:53:32+01:00\n",
       "135992    2023-03-20 17:01:10+01:00\n",
       "135993    2023-03-20 16:55:17+01:00\n",
       "135994    2023-03-20 17:03:25+01:00\n",
       "Name: clicks_created_at, Length: 135995, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# print the resulting dataframe\n",
    "df['clicks_created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't compare offset-naive and offset-aware datetimes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# create a new column 'before_after_change' and apply the function before_after_change\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# df['search_changes_fix.reordering.after.maximum.results'] = \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: before_after_change(row[\u001b[39m'\u001b[39;49m\u001b[39mclicks_created_at\u001b[39;49m\u001b[39m'\u001b[39;49m], search_engine_changes[\u001b[39m'\u001b[39;49m\u001b[39mfix.reordering.after.maximum.results\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9567\u001b[0m )\n\u001b[0;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[45], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# create a new column 'before_after_change' and apply the function before_after_change\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# df['search_changes_fix.reordering.after.maximum.results'] = \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: before_after_change(row[\u001b[39m'\u001b[39;49m\u001b[39mclicks_created_at\u001b[39;49m\u001b[39m'\u001b[39;49m], search_engine_changes[\u001b[39m'\u001b[39;49m\u001b[39mfix.reordering.after.maximum.results\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 4\u001b[0m, in \u001b[0;36mbefore_after_change\u001b[0;34m(clicks_created_at, change_at)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbefore_after_change\u001b[39m(clicks_created_at, change_at):\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mif\u001b[39;00m change_at \u001b[39m-\u001b[39m timedelta(weeks\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m clicks_created_at \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m change_at \u001b[39m+\u001b[39m timedelta(weeks\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mbefore_change\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m clicks_created_at \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m change_at \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mafter_change\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: can't compare offset-naive and offset-aware datetimes"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def before_after_change(clicks_created_at, change_at):\n",
    "    if change_at - timedelta(weeks=1) <= clicks_created_at <= change_at + timedelta(weeks=1):\n",
    "        return 'before_change' if clicks_created_at <= change_at else 'after_change'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# create a new column 'before_after_change' and apply the function before_after_change\n",
    "# df['search_changes_fix.reordering.after.maximum.results'] = \n",
    "df.apply(lambda row: before_after_change(row['clicks_created_at'], search_engine_changes['fix.reordering.after.maximum.results']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 1, 5, 11, 50)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine_changes['fix.reordering.after.maximum.results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
